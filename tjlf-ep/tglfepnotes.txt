This file exists just for creating an understanding of the parallels between the functions that TGLF-EP is calling from the Fortran TGLF. 
In the same way, we will be implementing certain functions in TJLF for use in the TJLF-EP code (some will not be translated, e.g. 
nfreya_driver.f90 etc.). There are a few trees that have already been created by Dr. Neiser that depict the branches of certain 
TGLFEP_example.f90 files which I will start with in order to better understand what TGLF-EP is both doing and how to better translate 
it into Julia.

TGLFEP_ky.f90:
    Uses:
        get_QL_energy_flux
        get_QL_particle_flux
        get_frequency
        get_growthrate
        get_wavefunction_out
        tglf_as_in
        tglf_rlns_in
        tglf_rlts_in
        tglf_run
        tglf_taus_in
        TGLFEP_tglf_map.f90
    Used by:
        TGLFEF_scalefactor.f90
        TGLFEP_ky_widthscan.f90
        TGLFEP_ky_nEPscan.f90
        TGLFEP_kwscale_scan.f90
        TGLFEP_mainsub.f90
        TGLFEP_driver.f90

    - All of the get_ex... functions are relatively straightforward and are all results of the TGLF
    run, hence why this function calls TGLF_run. It also calls TGLFEP_tglf_map beforehand, which is not
    indicated in the tree as a dependency but a called function. This TGLFEP_tglf_map will also need to be 
    mapped and translated.
    
The module TGLFEP_interface contains the definition of the module for the input.TGLFEP file. There are a few extra listed variables to 
consider that aren't in the input.TGLFEP file, possibly due to being defined in the map or other files. Similar to TJLF alone, I think it 
will be best if I can directly extract the material from the input files (TGLFEP and gacode) rather than mess with creating a Julia 
interface or calling TGLF methods, e.g.. My mapping subroutine will follow the same fashion as TGLFEP with mapping parameters to TGLF. 
As TJLF already has a way to translate TGLF to TJLF, I don't see any reason to make a new mapping from TJLFEP to TJLF instead.

Essentially, in Julia, I need to create an equivalent function that takes the input.gacode and input.TGLFEP files and map what is needed 
to an inputTGLF struct. This can then be used in the process defined by TJLF_read_input.jl. 

I need to better understand how the .TGLFEP and .profile inputs are being mapped to TGLF so as to organize this code. Are .profile 
generated from .gacode? .TGLFEP is developed in the driver of TGLFEP. .profile is explicitly developed in the tglf_map, so the profile 
is clearly more important (it certainly must develop from gacode as TGLFEP input will just be used for the purposes of TGLFEP). 
In that case, I will need to define an inputTGLF struct for mapping purposes and some kind of struct for TGLFEP inputs. input.profile will 
be mapped into TGLF as in fortran but any important variables will be stored in a TGLFEP struct. The inputTGLF may then be translated 
and ran in TJLF, the results of which will be used for the rest of TJLFEP. This may be a little too simplistic or missing something that 
I must consider, but that is my understanding as of now from reading driver.f90, interface.f90, tglf_map.f90.... This is one of the more 
important things to get right at the beginning, the structure of the data I will be using so as to effectively use Julia. This also follows 
in the footsteps of the previous TJLF development.

2/6:

    So the mapping will go as follows: the input.TGLFEP file will be dissected (I've already written most of this code but
    I need to figure out if there's any componenets to the InputTJLFEP struct I've made) and then theh input.gacode will be dissected into
    an InputTJLF struct with the function meant for such mapping. TGFLEP itself has a subroutine for EXPRO and direct reading (EXPRO I cannot seem to 
    find much info on. Is it related to Alpha?)

    The mapping of the gacode input is what is confusing me the most at this point. When you have a .profile file to work with, this makes the
    whole process so straightforward from the perspective of the fortran code. 

2/22:

    There have been quite a lot of updates since the previous time I wrote in this notes file. I want to track a few of those and maybe eventually commit this as a journal
    for any new student who is wondering what I've attempted to do. Firstly, TJLFEP follows a very similar method to TGLFEP right now with the most obvious exception being the
    existence of the input.MTGLF and input.EXPRO files in the tglfep_tests folder. input.MTGLF exists as a proxy for the input.profile or input.gacode that TGLFEP takes as an input.
    The reason for its existence is because TJLF itself reads an input.TGLF file; the input.profile method is extremely similar to this. The problem is that input.profile is largely
    not what is used for many of the cases (INPUT_PROFILE_METHOD == 2 requires an input.gacode). input.MTGLF is created by running the altered Fortran TGLFEP on your gacode of interest
    and it produces a new input file which displays all needed info in a long "="-delimited format (~8500 lines per input.MTGLF for NR = 201 (number of radii)). tjlfep_read_inputs.jl has
    multiple functions which are meant to parse the input.TGLFEP (this one is the same as TGLFEP's input.TGLFEP) and the aforementioned input.MTGLF. As for the input.EXPRO, this is also
    extrapolated from the Fortran, but is a constant and is meant to substitute for the fact that TGLFEP requires the Fortran code "EXPRO". The constants can be taken from this in the same
    way as input.MTGLF.

    driver.f90 in TGLFEP is not independently represented in TJLFEP. It is incorporated as part of main.jl. As up to now, the main focus is for PROCESS_IN = 5, here is the pathway that 
    the program will follow given an appropriate input:
        main.jl ==> mainsub.jl (case 5) ==> tjlf_kwscale_scan.jl =loops over=> tjlfep_ky.jl =runs=> run_tjlf.jl (from TJLF)
    
    One important thing that I haven't mentioned up to this point is that TJLFEP will also integrate MPI as TGLFEP did. MPI is essentially a package which divides up processes between
    processors in order to more quickly execute the code. These processes are first split in the main.jl and are then subsequently ran on all of the following processes. There are 
    breakpoints (MPI.Barrier()) which require all processes to finish before continuing within the code. In tjlf_kwscale_scan.jl, you can see this happen right before all of the 
    MPI processes are then combined with ALL_REDUCE.

    

